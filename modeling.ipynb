{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from data_splitter import data_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from face_detection import process_images\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from face_detection_2 import perform_face_detection\n",
    "\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "warnings.filterwarnings(\"default\", category=DeprecationWarning, module=__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getcwd()\n",
    "input_folder = f'{PATH}/../datasets_directory/dataset'\n",
    "output_folder = f'{PATH}/../datasets_directory/splitted_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Preparation\n",
    "<p>Preprocessing the image dataset before training a black and white face detection model is critical for several technical reasons that enhance model quality and performance:</p>\n",
    "<ol>\n",
    "<li>Data Quality Assurance: Removing low-quality or faceless images ensures the training dataset is high-quality, reducing the risk of overfitting to noisy data.\n",
    "</li><br/>\n",
    "<li>\n",
    "Noise Reduction: Eliminating non-face images or low-quality faces reduces dataset noise, improving the model's ability to generalize.\n",
    "</li><br/>\n",
    "<li>\n",
    "Bias Mitigation: Balancing the dataset representation mitigates bias, ensuring a more equitable model.\n",
    "</li><br/>\n",
    "<li>\n",
    "Robust Feature Learning: High-quality data helps the model learn robust and discriminative features for accurate face detection.\n",
    "</li><br/>\n",
    "<li>\n",
    "Alignment and Consistency:\n",
    "Face detection during preprocessing may involve alignment and normalization, ensuring that all face regions are consistently positioned and scaled.\n",
    "Consistent alignment helps the model learn invariant features, making it more adaptable to different poses and perspectives of black and white faces.\n",
    "</li><br/>\n",
    "<li>\n",
    "Alignment and Consistency: Consistent alignment aids the model in handling different face poses and perspectives.\n",
    "</li><br/>\n",
    "</li>\n",
    "<li>Training Efficiency: Removing irrelevant images speeds up training, benefiting model development.\n",
    "</li><br/>\n",
    "<li>\n",
    "Enhanced Generalization: A clean dataset fosters better model generalization to real-world scenarios.\n",
    "</li><br/>\n",
    "<li>Ethical Compliance: Ensuring the dataset is free from inappropriate content aligns with ethical AI development.\n",
    "</li>\n",
    "<br/><br/>\n",
    "<p>\n",
    "In summary, the process of exploring, cleaning, and filtering the dataset for black and white face detection using the DeepFace library is essential to enhance data quality, mitigate bias, improve model generalization, and ensure ethical AI development. It aligns with best practices in data preprocessing and contributes to the overall success of the model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_mapping = {\n",
    "    'white_faces': 'black_faces',\n",
    "    'black_faces': 'white_faces'\n",
    "}\n",
    "\n",
    "for input_folder, output_folder in folder_mapping.items():\n",
    "    input_path = f'{PATH}/../datasets_directory/dataset/{input_folder}'\n",
    "    # output_path = f'{PATH}/../datasets_directory/detected_face_dataset/{output_folder}'\n",
    "    # perform_face_detection(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 25111 files [00:18, 1337.11 files/s]\n"
     ]
    }
   ],
   "source": [
    "main_data = f'{PATH}/../datasets_directory/detected_face_dataset'\n",
    "split_data = f'{PATH}/../datasets_directory/splitted_dataset'\n",
    "data_split(main_data, split_data, 0.7, 0.15, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Counts:\n",
      "\n",
      "Total Train images:\n",
      "\tWhite_faces: 12250\n",
      "\tBlack_faces: 12099\n",
      "Total Val images:\n",
      "\tWhite_faces: 3664\n",
      "\tBlack_faces: 3580\n",
      "Total Test images:\n",
      "\tWhite_faces: 3686\n",
      "\tBlack_faces: 3576\n",
      "------------------------\n",
      "Total training images: 24349\n",
      "Total validation images: 7244\n",
      "Total test images: 7262\n"
     ]
    }
   ],
   "source": [
    "categories = ['white_faces', 'black_faces']\n",
    "subsets = ['train', 'val', 'test']\n",
    "output_folder = f'{PATH}/../datasets_directory/splitted_dataset'\n",
    "data_location_map = {\n",
    "    'train_dir': os.path.join(output_folder, \"train\"),\n",
    "    'val_dir': os.path.join(output_folder, \"val\"),\n",
    "    'test_dir': os.path.join(output_folder, \"test\")\n",
    "}\n",
    "\n",
    "data_counts = {'train': {}, 'val': {}, 'test': {}}\n",
    "\n",
    "for subset in subsets:\n",
    "    subset_dir = os.path.join(output_folder, subset)\n",
    "    for category in categories:\n",
    "        category_dir = os.path.join(subset_dir, category)\n",
    "        num_images = len(os.listdir(category_dir))\n",
    "        data_counts[subset][category] = num_images\n",
    "\n",
    "print(\"Data Counts:\\n\")\n",
    "for subset, categories_data in data_counts.items():\n",
    "    print(f\"Total {subset.capitalize()} images:\")\n",
    "    for category, count in categories_data.items():\n",
    "        print(f\"\\t{category.capitalize()}: {count}\")\n",
    "\n",
    "# Calculate totals\n",
    "total_train = sum(data_counts['train'].values())\n",
    "total_val = sum(data_counts['val'].values())\n",
    "total_test = sum(data_counts['test'].values())\n",
    "\n",
    "print(\"------------------------\")\n",
    "print(\"Total training images:\", total_train)\n",
    "print(\"Total validation images:\", total_val)\n",
    "print(\"Total test images:\", total_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of epochs for training\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# Define image dimensions (height and width) for data preprocessing\n",
    "IMG_HEIGHT = 86\n",
    "IMG_WIDTH = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackwhitefacedetector-eFQwdokx-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
